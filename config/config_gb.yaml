Pretraining:
  training:
    n_epochs: 30
    n_cores: 28
    device: "cuda"
    patience: -1
    swa_lrs: -1
    batch_size: 4
    max_len: 100000
    n_warmup_steps: 40000
    n_cycles: 0.5
    weight_decay: 0.1
    learning_rate: 0.0003
    save_every: 2000
  SwanDNA:
    input_size: 5
    embedding_size: 144
    max_len: 100000
    group_size: 8
    hidden_size: 256
    mlp_dropout: 0
    layer_dropout: 0
    prenorm: None
    norm: None
Human_Promoter:
  training:
    pretrained: True
    batch_size: 1024
    n_warmup_steps: 50000
    n_cycles: 0.5
    weight_decay: 0.1
    learning_rate: 0.003
    save_every: 2500
    n_epochs: 300
    device: "cuda"
    patience: -1
    swa_lrs: -1
  SwanDNA:
    input_size: 5
    output_size: 2
    embedding_size: 144
    max_len: 251
    group_size: 24
    hidden_size: 256
    mlp_dropout: 0
    layer_dropout: 0
    prenorm: "None"
    norm: "None"
    coeff: 1.2
Human_Enhancers_Cohn:
  training:
    pretrained: True
    batch_size: 1024
    n_warmup_steps: 50000
    n_cycles: 0.5
    weight_decay: 0.1
    learning_rate: 0.00008
    save_every: 2500
    n_epochs: 800
    device: "cuda"
    patience: -1
    swa_lrs: -1
  SwanDNA:
    input_size: 5
    output_size: 2
    embedding_size: 144
    max_len: 500
    group_size: 24
    hidden_size: 256
    mlp_dropout: 0
    layer_dropout: 0
    prenorm: "None"
    norm: "None"
    coeff: 1.3
Demo_Human_Or_Worm:
  training:
    pretrained: True
    batch_size: 1024
    n_warmup_steps: 50000
    n_cycles: 0.5
    weight_decay: 0.1
    learning_rate: 0.0001
    save_every: 2500
    n_epochs: 200
    device: "cuda"
    patience: -1
    swa_lrs: -1
  SwanDNA:
    input_size: 5
    output_size: 2
    embedding_size: 144
    max_len: 100
    group_size: 24
    hidden_size: 256
    mlp_dropout: 0
    layer_dropout: 0
    prenorm: "None"
    norm: "None"
    coeff: 1.2
Demo_Mouse_Enhancers:
  training:
    pretrained: True
    batch_size: 128
    n_warmup_steps: 50000
    n_cycles: 0.5
    weight_decay: 0.1
    learning_rate: 0.0005
    save_every: 2500
    n_epochs: 200
    device: "cuda"
    patience: -1
    swa_lrs: -1
  SwanDNA:
    input_size: 5
    output_size: 2
    embedding_size: 144
    max_len: 4776
    group_size: 18
    hidden_size: 256
    mlp_dropout: 0
    layer_dropout: 0
    prenorm: "None"
    norm: "None"
    coeff: 1.2
Demo_Coding_Inter:
  training:
    pretrained: True
    batch_size: 1024
    n_warmup_steps: 50000
    n_cycles: 0.5
    weight_decay: 0.1
    learning_rate: 0.001
    save_every: 2500
    n_epochs: 80
    device: "cuda"
    patience: -1
    swa_lrs: -1
  SwanDNA:
    input_size: 5
    output_size: 2
    embedding_size: 144
    max_len: 200
    group_size: 24
    hidden_size: 256
    mlp_dropout: 0
    layer_dropout: 0
    prenorm: "None"
    norm: "None"
    coeff: 1.2
Human_Enhancers_Ensembl:
  training:
    pretrained: True
    batch_size: 1024
    n_warmup_steps: 50000
    n_cycles: 0.5
    weight_decay: 0.1
    learning_rate: 0.0001
    save_every: 2500
    n_epochs: 80
    device: "cuda"
    patience: -1
    swa_lrs: -1
  SwanDNA:
    input_size: 5
    output_size: 2
    embedding_size: 144
    max_len: 573
    group_size: 16
    hidden_size: 256
    mlp_dropout: 0
    layer_dropout: 0
    prenorm: "None"
    norm: "None"
    coeff: 1.2
Human_Regulatory:
  training:
    pretrained: True
    batch_size: 1024
    n_warmup_steps: 50000
    n_cycles: 0.5
    weight_decay: 0.1
    learning_rate: 0.0001
    save_every: 2500
    n_epochs: 400
    device: "cuda"
    patience: -1
    swa_lrs: -1
  SwanDNA:
    input_size: 5
    output_size: 3
    embedding_size: 144
    max_len: 802
    group_size: 18
    hidden_size: 256
    mlp_dropout: 0
    layer_dropout: 0
    prenorm: "None"
    norm: "None"
    coeff: 1.2
Human_Ocr_Ensembl:
  training:
    pretrained: True
    batch_size: 1024
    n_warmup_steps: 50000
    n_cycles: 0.5
    weight_decay: 0.1
    learning_rate: 0.003
    save_every: 2500
    n_epochs: 40
    device: "cuda"
    patience: -1
    swa_lrs: -1
  SwanDNA:
    input_size: 5
    output_size: 2
    embedding_size: 72
    max_len: 593
    group_size: 12
    hidden_size: 256
    mlp_dropout: 0
    layer_dropout: 0
    prenorm: "None"
    norm: "None"
    coeff: 1.2
